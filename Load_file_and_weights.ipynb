{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import os\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################### LOADING DATA ##############################\n",
    "\n",
    "### Create a dictionary that contains all the dictiories within each dataset (workspace) we want to load ###\n",
    "\n",
    "#Go to the data directory\n",
    "dataDir = \"C:/Liliana/DATA_CNN_networks/\"\n",
    "\n",
    "Load_Data = {}  \n",
    "for i in range(0, len(os.listdir( dataDir ))):  \n",
    "    Load_Data[i] = {}  #Dictionary for a single workspace\n",
    "\n",
    "    \n",
    "\n",
    "### lOAD THE DATA STORING THEM IN A DICTIONARY ###\n",
    "\n",
    "i = 0\n",
    "for file in os.listdir( dataDir ):\n",
    "    Load_Data[i]['File'] =  scipy.io.loadmat( dataDir+file )\n",
    "    i = i+1\n",
    "    \n",
    "#print(All_Data[0]['File']['decisionlist'])\n",
    "#print(All_Data[1]['File']['RESULTS']['STATISTICS'][0,0]['TotalNumCells'][0])\n",
    "#print(All_Data[1]['File']['RAWDATA'][0,0]['files'][0,0]['coords_x'].astype(double))\n",
    "#print(All_Data[1]['File']['RAWDATA'][0,0]['files'][0,0][0]['intensity'][0])  #intensity curve for first cell\n",
    "#print(All_Data[1]['File']['decisionlist'])  #decision list\n",
    "\n",
    "\n",
    "#### Define another dictionary for storing intensity curves and classes ### \n",
    "#From each dataset, the intensity curve and classes are extracted and stored in another dictionary\n",
    "\n",
    "DATA = {}\n",
    "N_of_file =len(os.listdir( dataDir ))\n",
    "for i in range(0, N_of_file):\n",
    "        DATA[i] = {}\n",
    "\n",
    "\n",
    "\n",
    "#### Fill the dictionary ###\n",
    "\n",
    "Ncell_classified = 200 #Number of classified cells per dataset\n",
    "for i in range(0, N_of_file):\n",
    "    #Decision array for each dataset\n",
    "    DATA[i]['decision'] = Load_Data[i]['File']['decisionlist']\n",
    "    #Size of the matrix to store intensity values\n",
    "    NumofData = len(Load_Data[i]['File']['RAWDATA'][0,0]['files'][0,0][0]['intensity'])\n",
    "    Int_curve_len = len(Load_Data[4]['File']['RAWDATA'][0,0]['files'][0,0][0]['intensity'][0][0])\n",
    "    #Empty matrix to store all the intensity values of all the cells\n",
    "    DATA[i]['intensity'] = np.zeros((Ncell_classified,  Int_curve_len ))\n",
    "    for k in range(0,Ncell_classified): #Only the first 200 were classified\n",
    "        DATA[i]['intensity'][k,:] = Load_Data[i]['File']['RAWDATA'][0,0]['files'][0,0][0]['intensity'][k][0,0:900] #copy the first 900 values\n",
    "\n",
    "        \n",
    "### Classes array: contain values from all the data ###\n",
    "Classes=[]\n",
    "for i in range( 0, N_of_file ):\n",
    "    Classes = np.append(Classes , DATA[i]['decision']).astype(int)\n",
    "\n",
    "    \n",
    "### Intensities matrix: contain values from all the data    \n",
    "NTot_Data = N_of_file*Ncell_classified\n",
    "Intensity = np.zeros((0, DATA[0]['intensity'].shape[1]))\n",
    "for i in range( 0, N_of_file ):\n",
    "    Intensity = np.append(Intensity, DATA[i]['intensity'][:,:], axis=0)\n",
    "\n",
    "    \n",
    "### Elimate intensity profiles with NaN values and corresponding classes ###\n",
    "nan_indices = np.where(np.isnan(Intensity))\n",
    "Intensity = np.delete(Intensity,nan_indices[0], axis = 0)\n",
    "Classes = np.delete(Classes,nan_indices[0], axis = 0)\n",
    "#Redefine the total number of data\n",
    "total = len(Intensity)\n",
    "\n",
    "\n",
    "### Downsampling ###\n",
    "data1 = Intensity[:,0:896]\n",
    "step = 7\n",
    "data_downsampl = data1[:,::step]\n",
    "#Redefine len of the Intensity curves:\n",
    "Int_profile_len = len(data_downsampl[0])\n",
    "\n",
    "\n",
    "### Distribute all values randomly ###\n",
    "randseq = np.random.choice(np.arange(0,total),total,replace=False)\n",
    "#Data randomly organized\n",
    "data_downsampl = data_downsampl[randseq,:]\n",
    "Classes = Classes[randseq]\n",
    "\n",
    "######################### load file - training data ####################\n",
    "train_dt = data_downsampl[0:800,:]\n",
    "train_cl = Classes[0:800]\n",
    "X = train_dt  \n",
    "y = train_cl \n",
    "N_train_cell = len(train_dt)\n",
    "# process the data to fit in a keras CNN properly\n",
    "# input data needs to be (N, C, X, Y) - shaped where\n",
    "# N - number of samples\n",
    "# C - number of channels per sample\n",
    "# (X, Y) - sample size\n",
    "\n",
    "X = X.reshape((N_train_cell, Int_profile_len,1, 1))\n",
    "y = np_utils.to_categorical(y)\n",
    "\n",
    "\n",
    "######################### load file - testing data ####################\n",
    "#Define variables \n",
    "N_test_cell = total - N_train_cell\n",
    "#Matrix for intensity values\n",
    "test_dt = np.zeros((N_test_cell,Int_profile_len))\n",
    "#Use as testing data the last N_test_cell values within the total \n",
    "test_dt[:,:]= data_downsampl[N_train_cell:total]\n",
    "#Same process for the associated classes\n",
    "test_cl = Classes[N_train_cell:total]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How to load and use weights from a checkpoint\n",
    "\n",
    "# define a CNN\n",
    "# see http://keras.io for API reference\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Convolution2D(64, (3, 1),activation=\"relu\",input_shape=(Int_profile_len,1, 1),padding=\"same\"))\n",
    "cnn.add(Convolution2D(64, (3, 1), activation=\"relu\", padding=\"same\"))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,1)))\n",
    "\n",
    "#border_mode = same => output size is the same of input size. The filter does not go outside the bounds of the input\n",
    "#ReLU => thresholding at zero. All the negative values will be = 0, values bigger than zero will mantain the same value.\n",
    "cnn.add(Convolution2D(128, (3, 1), activation=\"relu\", padding=\"same\"))\n",
    "cnn.add(Convolution2D(128, (3, 1), activation=\"relu\", padding=\"same\"))\n",
    "cnn.add(Convolution2D(128, (3, 1), activation=\"relu\", padding=\"same\"))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,1)))\n",
    "    \n",
    "cnn.add(Convolution2D(256, (3, 1), activation=\"relu\", padding=\"same\"))\n",
    "cnn.add(Convolution2D(256, (3, 1), activation=\"relu\", padding=\"same\"))\n",
    "cnn.add(Convolution2D(256, (3, 1), activation=\"relu\", padding=\"same\"))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,1)))\n",
    "    \n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(1024, activation=\"relu\"))\n",
    "cnn.add(Dropout(0.5)) \n",
    "cnn.add(Dense(6, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model and loaded weights from file\n"
     ]
    }
   ],
   "source": [
    "# load weights\n",
    "cnn.load_weights(\"weights.best.hdf5\")\n",
    "# Compile model (required to make predictions)\n",
    "cnn.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0), metrics=['accuracy'])\n",
    "print(\"Created model and loaded weights from file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.63157894736842\n"
     ]
    }
   ],
   "source": [
    "# estimate accuracy on whole dataset using loaded weights\n",
    "out = cnn.predict(test_dt.reshape((N_test_cell, Int_profile_len,1, 1)))  \n",
    "print((float(np.sum(np.argmax(out,1)==test_cl))/float(test_cl.shape[0])*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
