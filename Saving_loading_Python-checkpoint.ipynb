{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liliana\\Anaconda2\\envs\\myenv2\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:161: UserWarning: pylab import has clobbered these variables: ['step']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import os\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###################################### LOADING DATA ##############################\n",
    "\n",
    "### Create a dictionary that contains all the dictiories within each dataset (workspace) we want to load ###\n",
    "\n",
    "#Go to the data directory\n",
    "dataDir = \"C:/Liliana/DATA_CNN_networks/\"\n",
    "\n",
    "Load_Data = {}  \n",
    "for i in range(0, len(os.listdir( dataDir ))):  \n",
    "    Load_Data[i] = {}  #Dictionary for a single workspace\n",
    "\n",
    "    \n",
    "\n",
    "### lOAD THE DATA STORING THEM IN A DICTIONARY ###\n",
    "\n",
    "i = 0\n",
    "for file in os.listdir( dataDir ):\n",
    "    Load_Data[i]['File'] =  scipy.io.loadmat( dataDir+file )\n",
    "    i = i+1\n",
    "    \n",
    "#print(All_Data[0]['File']['decisionlist'])\n",
    "#print(All_Data[1]['File']['RESULTS']['STATISTICS'][0,0]['TotalNumCells'][0])\n",
    "#print(All_Data[1]['File']['RAWDATA'][0,0]['files'][0,0]['coords_x'].astype(double))\n",
    "#print(All_Data[1]['File']['RAWDATA'][0,0]['files'][0,0][0]['intensity'][0])  #intensity curve for first cell\n",
    "#print(All_Data[1]['File']['decisionlist'])  #decision list\n",
    "\n",
    "\n",
    "#### Define another dictionary for storing intensity curves and classes ### \n",
    "#From each dataset, the intensity curve and classes are extracted and stored in another dictionary\n",
    "\n",
    "DATA = {}\n",
    "N_of_file =len(os.listdir( dataDir ))\n",
    "for i in range(0, N_of_file):\n",
    "        DATA[i] = {}\n",
    "\n",
    "\n",
    "\n",
    "#### Fill the dictionary ###\n",
    "\n",
    "Ncell_classified = 200 #Number of classified cells per dataset\n",
    "\n",
    "for i in range(0, N_of_file):\n",
    "    #Decision array for each dataset\n",
    "    DATA[i]['decision'] = Load_Data[i]['File']['decisionlist']\n",
    "    \n",
    "    #Size of the matrix to store intensity values\n",
    "    NumofData = len(Load_Data[i]['File']['RAWDATA'][0,0]['files'][0,0][0]['intensity'])\n",
    "    Int_curve_len = len(Load_Data[4]['File']['RAWDATA'][0,0]['files'][0,0][0]['intensity'][0][0])\n",
    "    \n",
    "    #Empty matrix to store all the intensity values of all the cells\n",
    "    DATA[i]['intensity'] = np.zeros((Ncell_classified,  Int_curve_len ))\n",
    "    \n",
    "    for k in range(0,Ncell_classified): #Only the first 200 were classified\n",
    "        DATA[i]['intensity'][k,:] = Load_Data[i]['File']['RAWDATA'][0,0]['files'][0,0][0]['intensity'][k][0,0:900] #copy the first 900 values\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "### Classes array: contain values from all the data ###\n",
    "\n",
    "Classes=[]\n",
    "for i in range( 0, N_of_file ):\n",
    "    Classes = np.append(Classes , DATA[i]['decision']).astype(int)\n",
    "\n",
    "### Intensities matrix: contain values from all the data    \n",
    "NTot_Data = N_of_file*Ncell_classified\n",
    "\n",
    "Intensity = np.zeros((0, DATA[0]['intensity'].shape[1]))\n",
    "for i in range( 0, N_of_file ):\n",
    "    Intensity = np.append(Intensity, DATA[i]['intensity'][:,:], axis=0)\n",
    "    \n",
    "    \n",
    "print(Classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 900)\n",
      "(990,)\n"
     ]
    }
   ],
   "source": [
    "### Elimate intensity profiles with NaN values and corresponding classes ###\n",
    "\n",
    "nan_indices = np.where(np.isnan(Intensity))\n",
    "\n",
    "Intensity = np.delete(Intensity,nan_indices[0], axis = 0)\n",
    "Classes = np.delete(Classes,nan_indices[0], axis = 0)\n",
    "\n",
    "print(Intensity.shape)\n",
    "print(Classes.shape)\n",
    "\n",
    "#Redefine the total number of data\n",
    "total = len(Intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 896)\n",
      "data shape (990, 128)\n",
      "128\n",
      "(990, 128)\n"
     ]
    }
   ],
   "source": [
    "### Downsampling ###\n",
    "data1 = Intensity[:,0:896]\n",
    "print(data1.shape)\n",
    "\n",
    "step = 7\n",
    "data_downsampl = data1[:,::step]\n",
    "print('data shape', data_downsampl.shape)\n",
    "\n",
    "#Redefine len of the Intensity curves:\n",
    "Int_profile_len = len(data_downsampl[0])\n",
    "print(Int_profile_len)\n",
    "print(data_downsampl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(Classes)):\n",
    "    if Classes[i] == 22:\n",
    "        print(i)\n",
    "    elif Classes[i] == 23:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 990\n",
      "(990,)\n",
      "(990,)\n",
      "(990, 128)\n"
     ]
    }
   ],
   "source": [
    "### Distribute all values randomly ###\n",
    "\n",
    "print('total',total)\n",
    "randseq = np.random.choice(np.arange(0,total),total,replace=False)\n",
    "\n",
    "print(randseq.shape)\n",
    "#Data randomly organized\n",
    "data_downsampl = data_downsampl[randseq,:]\n",
    "Classes = Classes[randseq]\n",
    "\n",
    "print(Classes.shape)\n",
    "print(data_downsampl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (800,)\n",
      "(800, 128)\n",
      "N of training data 800\n",
      "len intensity 128\n",
      "(800, 6)\n"
     ]
    }
   ],
   "source": [
    "######################### load file - training data ####################\n",
    "\n",
    "train_dt = data_downsampl[0:800,:]\n",
    "train_cl = Classes[0:800]\n",
    "\n",
    "X = train_dt  \n",
    "y = train_cl \n",
    "\n",
    "\n",
    "print(train_dt.shape)\n",
    "N_train_cell = len(train_dt)\n",
    "print('N of training data', N_train_cell)\n",
    "print('len intensity', Int_profile_len)\n",
    "# process the data to fit in a keras CNN properly\n",
    "# input data needs to be (N, C, X, Y) - shaped where\n",
    "# N - number of samples\n",
    "# C - number of channels per sample\n",
    "# (X, Y) - sample size\n",
    "\n",
    "\n",
    "X = X.reshape((N_train_cell, Int_profile_len,1, 1))\n",
    "y = np_utils.to_categorical(y)\n",
    "\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_test_cell = 190\n",
      "(190, 128)\n",
      "(190,)\n"
     ]
    }
   ],
   "source": [
    "######################### load file - testing data ####################\n",
    "\n",
    "#Define variables \n",
    "N_test_cell = total - N_train_cell\n",
    "print('N_test_cell =', N_test_cell)\n",
    "\n",
    "#Matrix for intensity values\n",
    "test_dt = np.zeros((N_test_cell,Int_profile_len))\n",
    "\n",
    "#Use as testing data the last N_test_cell values within the total \n",
    "test_dt[:,:]= data_downsampl[N_train_cell:total]\n",
    "\n",
    "#Same process for the associated classes\n",
    "test_cl = Classes[N_train_cell:total]\n",
    "\n",
    "print(test_dt.shape)\n",
    "print(test_cl.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a CNN\n",
    "# see http://keras.io for API reference\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Convolution2D(64, (3, 1),activation=\"relu\",input_shape=(Int_profile_len,1, 1),padding=\"same\"))\n",
    "cnn.add(Convolution2D(64, (3, 1), activation=\"relu\", padding=\"same\"))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,1)))\n",
    "\n",
    "#border_mode = same => output size is the same of input size. The filter does not go outside the bounds of the input\n",
    "#ReLU => thresholding at zero. All the negative values will be = 0, values bigger than zero will mantain the same value.\n",
    "cnn.add(Convolution2D(128, (3, 1), activation=\"relu\", padding=\"same\"))\n",
    "cnn.add(Convolution2D(128, (3, 1), activation=\"relu\", padding=\"same\"))\n",
    "cnn.add(Convolution2D(128, (3, 1), activation=\"relu\", padding=\"same\"))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,1)))\n",
    "    \n",
    "cnn.add(Convolution2D(256, (3, 1), activation=\"relu\", padding=\"same\"))\n",
    "cnn.add(Convolution2D(256, (3, 1), activation=\"relu\", padding=\"same\"))\n",
    "cnn.add(Convolution2D(256, (3, 1), activation=\"relu\", padding=\"same\"))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,1)))\n",
    "    \n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(1024, activation=\"relu\"))\n",
    "cnn.add(Dropout(0.5)) \n",
    "cnn.add(Dense(6, activation=\"softmax\"))\n",
    "\n",
    "# define optimizer and objective, compile cnn\n",
    "\n",
    "cnn.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0))\n",
    "#default parameters coming from the paper: https://arxiv.org/abs/1412.6980v8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "7s - loss: 1.5179\n",
      "Epoch 2/40\n",
      "6s - loss: 1.0721\n",
      "Epoch 3/40\n",
      "6s - loss: 1.0077\n",
      "Epoch 4/40\n",
      "6s - loss: 0.9551\n",
      "Epoch 5/40\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    # train\n",
    "    cnn.fit(X, y, epochs=40, verbose = 2)\n",
    "    #Evaluation code which makes prediction\n",
    "    out = cnn.predict(test_dt.reshape((N_test_cell, Int_profile_len,1, 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
