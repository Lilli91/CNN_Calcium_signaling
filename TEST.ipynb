{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: val_acc improved from -inf to 0.67323, saving model to weights-improvement-00-0.67.hdf5\n",
      "Epoch 00001: val_acc did not improve\n",
      "Epoch 00002: val_acc improved from 0.67323 to 0.67717, saving model to weights-improvement-02-0.68.hdf5\n",
      "Epoch 00003: val_acc did not improve\n",
      "Epoch 00004: val_acc improved from 0.67717 to 0.67717, saving model to weights-improvement-04-0.68.hdf5\n",
      "Epoch 00005: val_acc did not improve\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 00009: val_acc improved from 0.67717 to 0.68898, saving model to weights-improvement-09-0.69.hdf5\n",
      "Epoch 00010: val_acc did not improve\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 00012: val_acc did not improve\n",
      "Epoch 00013: val_acc did not improve\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 00015: val_acc did not improve\n",
      "Epoch 00016: val_acc improved from 0.68898 to 0.70866, saving model to weights-improvement-16-0.71.hdf5\n",
      "Epoch 00017: val_acc did not improve\n",
      "Epoch 00018: val_acc did not improve\n",
      "Epoch 00019: val_acc did not improve\n",
      "Epoch 00020: val_acc did not improve\n",
      "Epoch 00021: val_acc did not improve\n",
      "Epoch 00022: val_acc did not improve\n",
      "Epoch 00023: val_acc did not improve\n",
      "Epoch 00024: val_acc did not improve\n",
      "Epoch 00025: val_acc did not improve\n",
      "Epoch 00026: val_acc did not improve\n",
      "Epoch 00027: val_acc did not improve\n",
      "Epoch 00028: val_acc did not improve\n",
      "Epoch 00029: val_acc did not improve\n",
      "Epoch 00030: val_acc did not improve\n",
      "Epoch 00031: val_acc did not improve\n",
      "Epoch 00032: val_acc did not improve\n",
      "Epoch 00033: val_acc did not improve\n",
      "Epoch 00034: val_acc did not improve\n",
      "Epoch 00035: val_acc did not improve\n",
      "Epoch 00036: val_acc did not improve\n",
      "Epoch 00037: val_acc did not improve\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-76fc421ff836>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[1;31m# Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.33\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Liliana\\Anaconda2\\envs\\myenv2\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32mC:\\Users\\Liliana\\Anaconda2\\envs\\myenv2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Liliana\\Anaconda2\\envs\\myenv2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Liliana\\Anaconda2\\envs\\myenv2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Liliana\\Anaconda2\\envs\\myenv2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Liliana\\Anaconda2\\envs\\myenv2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Liliana\\Anaconda2\\envs\\myenv2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\Liliana\\Anaconda2\\envs\\myenv2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Liliana\\Anaconda2\\envs\\myenv2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.data\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: val_acc improved from -inf to 0.67323, saving model to weights.best.hdf5\n",
      "Epoch 00001: val_acc did not improve\n",
      "Epoch 00002: val_acc did not improve\n",
      "Epoch 00003: val_acc improved from 0.67323 to 0.67323, saving model to weights.best.hdf5\n",
      "Epoch 00004: val_acc improved from 0.67323 to 0.68504, saving model to weights.best.hdf5\n",
      "Epoch 00005: val_acc improved from 0.68504 to 0.68898, saving model to weights.best.hdf5\n",
      "Epoch 00006: val_acc improved from 0.68898 to 0.69685, saving model to weights.best.hdf5\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 00010: val_acc did not improve\n",
      "Epoch 00011: val_acc improved from 0.69685 to 0.70079, saving model to weights.best.hdf5\n",
      "Epoch 00012: val_acc did not improve\n",
      "Epoch 00013: val_acc did not improve\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 00015: val_acc did not improve\n",
      "Epoch 00016: val_acc did not improve\n",
      "Epoch 00017: val_acc did not improve\n",
      "Epoch 00018: val_acc improved from 0.70079 to 0.70866, saving model to weights.best.hdf5\n",
      "Epoch 00019: val_acc did not improve\n",
      "Epoch 00020: val_acc did not improve\n",
      "Epoch 00021: val_acc did not improve\n",
      "Epoch 00022: val_acc did not improve\n",
      "Epoch 00023: val_acc did not improve\n",
      "Epoch 00024: val_acc did not improve\n",
      "Epoch 00025: val_acc did not improve\n",
      "Epoch 00026: val_acc did not improve\n",
      "Epoch 00027: val_acc did not improve\n",
      "Epoch 00028: val_acc improved from 0.70866 to 0.71654, saving model to weights.best.hdf5\n",
      "Epoch 00029: val_acc did not improve\n",
      "Epoch 00030: val_acc did not improve\n",
      "Epoch 00031: val_acc did not improve\n",
      "Epoch 00032: val_acc did not improve\n",
      "Epoch 00033: val_acc did not improve\n",
      "Epoch 00034: val_acc did not improve\n",
      "Epoch 00035: val_acc did not improve\n",
      "Epoch 00036: val_acc improved from 0.71654 to 0.71654, saving model to weights.best.hdf5\n",
      "Epoch 00037: val_acc did not improve\n",
      "Epoch 00038: val_acc improved from 0.71654 to 0.71654, saving model to weights.best.hdf5\n",
      "Epoch 00039: val_acc did not improve\n",
      "Epoch 00040: val_acc did not improve\n",
      "Epoch 00041: val_acc did not improve\n",
      "Epoch 00042: val_acc did not improve\n",
      "Epoch 00043: val_acc did not improve\n",
      "Epoch 00044: val_acc did not improve\n",
      "Epoch 00045: val_acc did not improve\n",
      "Epoch 00046: val_acc improved from 0.71654 to 0.72441, saving model to weights.best.hdf5\n",
      "Epoch 00047: val_acc did not improve\n",
      "Epoch 00048: val_acc did not improve\n",
      "Epoch 00049: val_acc did not improve\n",
      "Epoch 00050: val_acc did not improve\n",
      "Epoch 00051: val_acc did not improve\n",
      "Epoch 00052: val_acc did not improve\n",
      "Epoch 00053: val_acc did not improve\n",
      "Epoch 00054: val_acc did not improve\n",
      "Epoch 00055: val_acc did not improve\n",
      "Epoch 00056: val_acc did not improve\n",
      "Epoch 00057: val_acc did not improve\n",
      "Epoch 00058: val_acc did not improve\n",
      "Epoch 00059: val_acc did not improve\n",
      "Epoch 00060: val_acc did not improve\n",
      "Epoch 00061: val_acc did not improve\n",
      "Epoch 00062: val_acc did not improve\n",
      "Epoch 00063: val_acc improved from 0.72441 to 0.74016, saving model to weights.best.hdf5\n",
      "Epoch 00064: val_acc did not improve\n",
      "Epoch 00065: val_acc did not improve\n",
      "Epoch 00066: val_acc did not improve\n",
      "Epoch 00067: val_acc did not improve\n",
      "Epoch 00068: val_acc did not improve\n",
      "Epoch 00069: val_acc did not improve\n",
      "Epoch 00070: val_acc did not improve\n",
      "Epoch 00071: val_acc did not improve\n",
      "Epoch 00072: val_acc did not improve\n",
      "Epoch 00073: val_acc did not improve\n",
      "Epoch 00074: val_acc did not improve\n",
      "Epoch 00075: val_acc did not improve\n",
      "Epoch 00076: val_acc improved from 0.74016 to 0.74016, saving model to weights.best.hdf5\n",
      "Epoch 00077: val_acc did not improve\n",
      "Epoch 00078: val_acc did not improve\n",
      "Epoch 00079: val_acc did not improve\n",
      "Epoch 00080: val_acc did not improve\n",
      "Epoch 00081: val_acc did not improve\n",
      "Epoch 00082: val_acc did not improve\n",
      "Epoch 00083: val_acc improved from 0.74016 to 0.74409, saving model to weights.best.hdf5\n",
      "Epoch 00084: val_acc did not improve\n",
      "Epoch 00085: val_acc did not improve\n",
      "Epoch 00086: val_acc did not improve\n",
      "Epoch 00087: val_acc did not improve\n",
      "Epoch 00088: val_acc did not improve\n",
      "Epoch 00089: val_acc did not improve\n",
      "Epoch 00090: val_acc improved from 0.74409 to 0.75197, saving model to weights.best.hdf5\n",
      "Epoch 00091: val_acc did not improve\n",
      "Epoch 00092: val_acc did not improve\n",
      "Epoch 00093: val_acc did not improve\n",
      "Epoch 00094: val_acc did not improve\n",
      "Epoch 00095: val_acc did not improve\n",
      "Epoch 00096: val_acc did not improve\n",
      "Epoch 00097: val_acc did not improve\n",
      "Epoch 00098: val_acc improved from 0.75197 to 0.76772, saving model to weights.best.hdf5\n",
      "Epoch 00099: val_acc improved from 0.76772 to 0.77559, saving model to weights.best.hdf5\n",
      "Epoch 00100: val_acc did not improve\n",
      "Epoch 00101: val_acc did not improve\n",
      "Epoch 00102: val_acc did not improve\n",
      "Epoch 00103: val_acc did not improve\n",
      "Epoch 00104: val_acc improved from 0.77559 to 0.77953, saving model to weights.best.hdf5\n",
      "Epoch 00105: val_acc did not improve\n",
      "Epoch 00106: val_acc did not improve\n",
      "Epoch 00107: val_acc did not improve\n",
      "Epoch 00108: val_acc did not improve\n",
      "Epoch 00109: val_acc did not improve\n",
      "Epoch 00110: val_acc did not improve\n",
      "Epoch 00111: val_acc did not improve\n",
      "Epoch 00112: val_acc did not improve\n",
      "Epoch 00113: val_acc did not improve\n",
      "Epoch 00114: val_acc did not improve\n",
      "Epoch 00115: val_acc improved from 0.77953 to 0.79134, saving model to weights.best.hdf5\n",
      "Epoch 00116: val_acc did not improve\n",
      "Epoch 00117: val_acc did not improve\n",
      "Epoch 00118: val_acc did not improve\n",
      "Epoch 00119: val_acc did not improve\n",
      "Epoch 00120: val_acc did not improve\n",
      "Epoch 00121: val_acc did not improve\n",
      "Epoch 00122: val_acc did not improve\n",
      "Epoch 00123: val_acc did not improve\n",
      "Epoch 00124: val_acc did not improve\n",
      "Epoch 00125: val_acc did not improve\n",
      "Epoch 00126: val_acc did not improve\n",
      "Epoch 00127: val_acc did not improve\n",
      "Epoch 00128: val_acc did not improve\n",
      "Epoch 00129: val_acc did not improve\n",
      "Epoch 00130: val_acc did not improve\n",
      "Epoch 00131: val_acc did not improve\n",
      "Epoch 00132: val_acc did not improve\n",
      "Epoch 00133: val_acc did not improve\n",
      "Epoch 00134: val_acc did not improve\n",
      "Epoch 00135: val_acc did not improve\n",
      "Epoch 00136: val_acc did not improve\n",
      "Epoch 00137: val_acc improved from 0.79134 to 0.79921, saving model to weights.best.hdf5\n",
      "Epoch 00138: val_acc did not improve\n",
      "Epoch 00139: val_acc did not improve\n",
      "Epoch 00140: val_acc did not improve\n",
      "Epoch 00141: val_acc did not improve\n",
      "Epoch 00142: val_acc did not improve\n",
      "Epoch 00143: val_acc did not improve\n",
      "Epoch 00144: val_acc did not improve\n",
      "Epoch 00145: val_acc did not improve\n",
      "Epoch 00146: val_acc did not improve\n",
      "Epoch 00147: val_acc did not improve\n",
      "Epoch 00148: val_acc did not improve\n",
      "Epoch 00149: val_acc did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22fbfb96d30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checkpoint the weights for best model on validation accuracy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.data\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# checkpoint\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model and loaded weights from file\n",
      "acc: 77.34%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# How to load and use weights from a checkpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "# load weights\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "# Compile model (required to make predictions)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(\"Created model and loaded weights from file\")\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.data\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# estimate accuracy on whole dataset using loaded weights\n",
    "scores = model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
